 
# LLM-Finetuning-Research

## 1. 项目简介
 该仓库为一份大模型微调技术入门学习项目，旨在研究大型语言模型 (LLMs) 在垂直专业领域（如本研究中的“鞋类”领域）存在的知识缺陷问题。为满足从资源受限的终端设备到高性能云服务的不同应用需求，实现了一套“双框架”范式。

-  **框架一 (LAM)**：一个轻量级的语言模型实验，用于在低资源环境下深入探索和验证模型微调的核心机制。
-  **框架二 (服务化专家模型)**：一个面向实际应用的工程，构建了一个基于 Llama-3-8B 的LLM专家服务。

本仓库包含了复现这两个框架所有实验的完整代码、脚本和指南。

## 2. 双框架

### 框架一：轻量级适配模型 (Lightweight Adaptation Model, LAM)
-  **目标**: 其目的并非追求最佳性能，而是在严格控制变量的条件下，研究在极少样本和低算力下，不同微调范式（全量微调 vs. LoRA）和解码策略（贪心解码 vs. 随机采样）对模型知识注入、过拟合和泛化能力的内在影响。
-  **结论**: 实验证明，在低资源场景下，全量微调极易导致过拟合和泛化能力下降。相比之下，LoRA 结合贪心解码的策略，在确保知识高保真复现的同时，展现出更强的鲁棒性。


### 框架二：服务化领域专家模型
-  **目标**: 构建一个功能完备、性能可靠、可对外提供服务的领域专家系统。
-  **实现路径**: 该框架基于 Llama-3-8B 模型，应用了框架一中被验证为有效的 LoRA 微调策略。它整合了从数据处理、多阶段微调（SFT 知识注入 和 DPO 风格对齐）、服务化部署 (Model-as-a-Service)，到高级应用集成（RAG 实时知识更新 和 视觉辅助查询）的端到端全流程。


## 3. 项目结构
本仓库的整体结构如下，两个核心框架分别位于独立的子目录中：
```
├── framework_one/
│   ├── main.py
│   ├── model.py
│   ├── engine.py
│   ├── data_utils.py
│   ├── plot_utils.py
│   ├── config.py
│   └── README.md
│
├── framework_two/
│   ├── scripts/
│   │   ├── run_sft.sh
│   │   ├── run_dpo.sh
│   │   └── start_api.sh
│   ├── clients/
│   │   ├── rag_client.py
│   │   └── multimodal_rag_client.py
│   └── README.md
│
└── README.md      (\<- 您正在阅读的这个总览文件)
